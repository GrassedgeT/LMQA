import logging
import json
import concurrent.futures
from typing import List, Dict, Optional
from flask import current_app

# Adjust imports based on project structure
# Assuming 'backend' is the root context when running
try:
    from memory.manager import MemoryManager
except ImportError:
    # Fallback if running from a different context, though conventions say we should run from root or backend
    import sys
    import os
    sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '../../..')))
    from memory.manager import MemoryManager

from ..core.db import execute_query
from ..core.utils import decrypt_api_key

logger = logging.getLogger(__name__)

class AgentService:
    """æ™ºèƒ½ä½“æœåŠ¡é€‚é…å±‚ - é’ˆå¯¹ DeepSeek ä¼˜åŒ–çš„ Agentic æ¨¡å¼"""
    
    def __init__(self):
        self.memory_manager = None
        self.agent_service_url = None

    def init_app(self, app):
        self.agent_service_url = app.config.get('AGENT_SERVICE_URL')
        try:
            self.memory_manager = MemoryManager()
            logger.info("MemoryManager initialized successfully")
        except Exception as e:
            logger.error(f"Failed to initialize MemoryManager: {e}")
            self.memory_manager = None

    def _get_user_model_config(self, user_id: int) -> Optional[Dict]:
        try:
            config = execute_query('SELECT provider, model_name, api_key, base_url FROM user_model_configs WHERE user_id = ? AND is_default = 1 LIMIT 1', (user_id,))
            if config:
                config_dict = dict(config[0])
                config_dict['api_key'] = decrypt_api_key(config_dict['api_key'])
                return config_dict
            return None
        except Exception as e:
            logger.error(f'è·å–ç”¨æˆ·æ¨¡å‹é…ç½®å¤±è´¥: {str(e)}')
            return None

    def _get_llm_client(self, user_id: int):
        model_config = self._get_user_model_config(user_id)
        if not model_config: return None, None, None
        try:
            from openai import OpenAI
            client = OpenAI(api_key=model_config['api_key'], base_url=model_config['base_url'])
            return client, model_config['model_name'], model_config
        except Exception as e:
            logger.error(f'åˆ›å»º LLM å®¢æˆ·ç«¯å¤±è´¥: {str(e)}')
            return None, None, None

    def warm_up_for_user(self, user_id: int):
        try:
            config = self._get_user_model_config(user_id)
            if self.memory_manager: self.memory_manager.warm_up_client(config)
        except: pass

    # =========================================================================
    # 1. å·¥å…·å®šä¹‰ (åŠ å¼ºç‰ˆï¼šé˜²æ­¢æ¼è®°å§“å)
    # =========================================================================
    def _get_tools(self) -> List[Dict]:
        return [
            {
                "type": "function",
                "function": {
                    "name": "add_memory",
                    "description": "ä¿å­˜ç”¨æˆ·çš„é‡è¦ä¿¡æ¯ã€‚âš ï¸é‡è¦ï¼šå¦‚æœç”¨æˆ·åŒæ—¶æä¾›äº†ã€å§“å/èº«ä»½ã€‘å’Œã€å…¶ä»–äº‹å®ã€‘ï¼Œå¿…é¡»å°†å®ƒä»¬åˆå¹¶ä¿å­˜ï¼Œç»å¯¹ä¸èƒ½é—æ¼å§“åï¼",
                    "parameters": {
                        "type": "object",
                        "properties": {
                            "content": {
                                "type": "string",
                                "description": "è¦å­˜å‚¨çš„å®Œæ•´äº‹å®ã€‚å¿…é¡»åŒ…å«ä¸»è¯­ã€‚ä¾‹å¦‚ç”¨æˆ·è¯´'æˆ‘æ˜¯å°ç‹ï¼Œæœ‰ä¸ªåŒäº‹å«å°å¼ 'ï¼Œä½ å¿…é¡»å¡«å…¥ï¼š'ç”¨æˆ·å«å°ç‹ï¼Œç”¨æˆ·æœ‰ä¸€ä¸ªåŒäº‹å«å°å¼ ' (å¿…é¡»åŒ…å«ä¸¤ç‚¹)ã€‚"
                            }
                        },
                        "required": ["content"]
                    }
                }
            },
            {
                "type": "function",
                "function": {
                    "name": "search_memories",
                    "description": "æœç´¢å†å²è®°å¿†ã€‚",
                    "parameters": {
                        "type": "object",
                        "properties": {
                            "query": {
                                "type": "string",
                                "description": "æœç´¢å…³é”®è¯"
                            },
                            "limit": {
                                "type": "integer",
                                "default": 5
                            }
                        },
                        "required": ["query"]
                    }
                }
            }
        ]

    # =========================================================================
    # 2. System Prompt (åŠ å¼ºç‰ˆï¼šå…¨é‡å­˜å‚¨åŸåˆ™)
    # =========================================================================
    def _build_system_prompt(self) -> str:
        return """ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½åŠ©æ‰‹ï¼Œè´Ÿè´£ç®¡ç†ç”¨æˆ·è®°å¿†ã€‚

**è®°å¿†ç®¡ç†æœ€é«˜å‡†åˆ™ï¼š**
1. **å…¨é‡å­˜å‚¨ï¼ˆå…³é”®ï¼‰**ï¼šå½“ç”¨æˆ·ä¸€å¥è¯åŒ…å«å¤šä¸ªä¿¡æ¯ç‚¹ï¼ˆå°¤å…¶æ˜¯åŒ…å«â€œæˆ‘å«XXXâ€è¿™ç§èº«ä»½ä¿¡æ¯ï¼‰æ—¶ï¼Œ**å¿…é¡»**å°†æ‰€æœ‰ä¿¡æ¯åˆå¹¶åœ¨ä¸€æ¬¡ `add_memory` è°ƒç”¨ä¸­ã€‚
   - âŒ é”™è¯¯è¡Œä¸ºï¼šç”¨æˆ·è¯´â€œæˆ‘å«å°ç‹ï¼ŒåŒäº‹æ˜¯å°å¼ â€ï¼Œä½ åªå­˜â€œç”¨æˆ·æœ‰ä¸ªåŒäº‹å«å°å¼ â€ã€‚ï¼ˆæ¼æ‰äº†åå­—ï¼ï¼‰
   - âœ… æ­£ç¡®è¡Œä¸ºï¼šä½ è°ƒç”¨ `add_memory(content="ç”¨æˆ·å«å°ç‹ï¼Œç”¨æˆ·æœ‰ä¸€ä¸ªåŒäº‹å«å°å¼ ")`ã€‚

2. **ä¸»è¯­æ˜ç¡®**ï¼šDeepSeek/LLM è¯·æ³¨æ„ï¼ŒMem0 éœ€è¦æ˜ç¡®çš„ä¸»è¯­ã€‚
   - ä¸è¦è¯´ "æ˜¯ä¸ªç¨‹åºå‘˜"ã€‚
   - è¦è¯´ "ç”¨æˆ·æ˜¯ç¨‹åºå‘˜"ã€‚

3. **å…ˆæœåç­”**ï¼šå›ç­”é—®é¢˜å‰å…ˆæœç´¢ã€‚
"""

    # =========================================================================
    # 3. å·¥å…·æ‰§è¡Œ (ä¿æŒä¸å˜)
    # =========================================================================
    def _execute_tool(self, tool_name: str, tool_args: Dict, user_id: int, conversation_id: int, llm_settings: Dict) -> str:
        logger.info(f"ğŸ”§ Agent æ‰§è¡Œå·¥å…·: {tool_name} | å‚æ•°: {tool_args}")
        if not self.memory_manager: return "é”™è¯¯ï¼šè®°å¿†æ¨¡å—æœªåˆå§‹åŒ–ã€‚"

        try:
            if tool_name == "add_memory":
                res = self.memory_manager.add_memory(
                    content=tool_args["content"],
                    user_id=str(user_id),
                    run_id=None, # ä¿æŒå…¨å±€
                    metadata={"source_conversation_id": str(conversation_id)},
                    llm_settings=llm_settings
                )
                return "è®°å¿†å·²æ·»åŠ ã€‚"

            elif tool_name == "search_memories":
                res = self.memory_manager.search_memories(
                    query=tool_args["query"],
                    user_id=str(user_id),
                    limit=tool_args.get("limit", 5),
                    llm_settings=llm_settings
                )
                
                # Handle potential dict response (e.g. {'results': [...]})
                if isinstance(res, dict):
                    res_list = res.get("results", [])
                elif isinstance(res, list):
                    res_list = res
                else:
                    res_list = []

                memories = [m.get("memory", m.get("text", "")) for m in res_list if isinstance(m, dict)]
                return f"æœç´¢ç»“æœ: {json.dumps(memories, ensure_ascii=False)}"
            
            return f"æœªçŸ¥å·¥å…·: {tool_name}"
        except Exception as e:
            logger.error(f"å·¥å…·æ‰§è¡Œå¼‚å¸¸: {e}")
            return f"å·¥å…·æ‰§è¡Œå‡ºé”™: {str(e)}"

    # =========================================================================
    # 4. Agent Loop (ä¿æŒä¸å˜)
    # =========================================================================
    def chat_agent(self, user_id: int, conversation_id: int, user_message: str, history_messages: List[Dict]) -> str:
        client, model_name, llm_settings = self._get_llm_client(user_id)
        if not client: return "è¯·å…ˆé…ç½®æ¨¡å‹ API Keyã€‚"

        messages = [{"role": "system", "content": self._build_system_prompt()}]
        messages.extend(history_messages)
        messages.append({"role": "user", "content": user_message})

        tools = self._get_tools()
        max_turns = 5
        current_turn = 0
        
        while current_turn < max_turns:
            try:
                response = client.chat.completions.create(
                    model=model_name, messages=messages, tools=tools, tool_choice="auto", temperature=0.7
                )
                response_message = response.choices[0].message
                
                if response_message.tool_calls:
                    messages.append(response_message)
                    
                    # å¹¶è¡Œæ‰§è¡Œ
                    with concurrent.futures.ThreadPoolExecutor() as executor:
                        futures = []
                        for tool_call in response_message.tool_calls:
                            function_name = tool_call.function.name
                            try:
                                arguments = json.loads(tool_call.function.arguments)
                            except: arguments = {}
                            
                            future = executor.submit(
                                self._execute_tool,
                                function_name, arguments, user_id, conversation_id, llm_settings
                            )
                            futures.append((tool_call, future))
                        
                        for tool_call, future in futures:
                            tool_result = future.result()
                            messages.append({
                                "tool_call_id": tool_call.id, "role": "tool", 
                                "name": tool_call.function.name, "content": tool_result
                            })
                    
                    current_turn += 1
                else:
                    return response_message.content
            except Exception as e:
                logger.error(f"Agent Loop Error: {e}")
                return f"å¤„ç†é”™è¯¯: {str(e)}"
        
        return "æ€è€ƒè¶…æ—¶ã€‚"

    # --- å…¼å®¹æ–¹æ³• ---
    def delete_conversation_memories(self, *args): pass
    def search_memories(self, *args, **kwargs): return []
    def sync_memory(self, *args, **kwargs): return {}
    def update_memory(self, *args, **kwargs): pass
    def delete_memory(self, *args, **kwargs): pass
    def add_interaction(self, *args, **kwargs): pass
    def _process_message_stream_local(self, *args, **kwargs): pass

agent_service = AgentService()
