# backend/app/services/agent_service.py

import json
import logging
import concurrent.futures
import re
from typing import List, Dict, Optional, Union, Any
from flask import current_app

try:
    from memory.manager import MemoryManager
except ImportError:
    import sys
    import os
    sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '../../..')))
    from memory.manager import MemoryManager

from ..core.utils import decrypt_api_key
from ..core.db import execute_query
from openai import OpenAI

logger = logging.getLogger(__name__)

class AgentService:
    """æ™ºèƒ½ä½“æœåŠ¡ - Graph RAG (Vector + Graph) + å…¨åŸŸåŒæ­¥ä¸€è‡´æ€§åˆ é™¤"""
    
    def __init__(self):
        self.memory_manager = None
        self.agent_service_url = None

    def init_app(self, app):
        self.agent_service_url = app.config.get('AGENT_SERVICE_URL')
        try:
            self.memory_manager = MemoryManager()
            logger.info("MemoryManager initialized successfully via init_app")
        except Exception as e:
            logger.error(f"Failed to initialize MemoryManager: {e}")
            self.memory_manager = None

    def _get_user_model_config(self, user_id: int) -> Optional[Dict]:
        try:
            config = execute_query('SELECT provider, model_name, api_key, base_url FROM user_model_configs WHERE user_id = ? AND is_default = 1 LIMIT 1', (user_id,))
            if config:
                config_dict = dict(config[0])
                config_dict['api_key'] = decrypt_api_key(config_dict['api_key'])
                return config_dict
            return None
        except Exception as e:
            logger.error(f'è·å–ç”¨æˆ·æ¨¡å‹é…ç½®å¤±è´¥: {str(e)}')
            return None

    def _get_llm_client(self, user_id: int):
        model_config = self._get_user_model_config(user_id)
        if not model_config: return None, None, None
        try:
            client = OpenAI(api_key=model_config['api_key'], base_url=model_config['base_url'])
            return client, model_config['model_name'], model_config
        except Exception as e:
            logger.error(f'åˆ›å»º LLM Client å¤±è´¥: {str(e)}')
            return None, None, None

    def warm_up_for_user(self, user_id: int):
        try:
            config = self._get_user_model_config(user_id)
            if self.memory_manager: self.memory_manager.warm_up_client(config)
        except: pass

    # =========================================================================
    # 1. å·¥å…·å®šä¹‰
    # =========================================================================
    def _get_tools(self) -> List[Dict]:
        return [
            {
                "type": "function",
                "function": {
                    "name": "add_local_memory",
                    "description": "ã€å­˜å±€éƒ¨ã€‘ä¿å­˜ä»…ä¸å½“å‰å¯¹è¯ç›¸å…³çš„ç»†èŠ‚ã€‚",
                    "parameters": {
                        "type": "object",
                        "properties": {"content": {"type": "string", "description": "è®°å¿†å†…å®¹"}},
                        "required": ["content"]
                    }
                }
            },
            {
                "type": "function",
                "function": {
                    "name": "add_global_memory",
                    "description": "ã€å­˜å…¨å±€ã€‘ä¿å­˜ç”¨æˆ·çš„æ°¸ä¹…æ€§äº‹å®ã€‚ç³»ç»Ÿä¼šè‡ªåŠ¨æ›´æ–°çŸ¥è¯†å›¾è°±ã€‚",
                    "parameters": {
                        "type": "object",
                        "properties": {"content": {"type": "string", "description": "è®°å¿†å†…å®¹"}},
                        "required": ["content"]
                    }
                }
            },
            {
                "type": "function",
                "function": {
                    "name": "search_local_memories",
                    "description": "ã€æœå±€éƒ¨ã€‘åŒæ—¶è¿”å›æ–‡æœ¬è®°å¿†å’Œå›¾è°±å…³ç³»ã€‚",
                    "parameters": {
                        "type": "object",
                        "properties": {"query": {"type": "string", "description": "æœç´¢å…³é”®è¯"}},
                        "required": ["query"]
                    }
                }
            },
            {
                "type": "function",
                "function": {
                    "name": "search_global_memories",
                    "description": "ã€æœå…¨å±€ã€‘åŒæ—¶è¿”å›æ–‡æœ¬è®°å¿†å’Œå›¾è°±å…³ç³»ã€‚",
                    "parameters": {
                        "type": "object",
                        "properties": {"query": {"type": "string", "description": "æœç´¢å…³é”®è¯"}},
                        "required": ["query"]
                    }
                }
            },
            {
                "type": "function",
                "function": {
                    "name": "delete_memory",
                    "description": "ã€åˆ é™¤è®°å¿†ã€‘ç”¨æˆ·è¦æ±‚'å¿˜è®°'æˆ–'åˆ é™¤'æ—¶ä½¿ç”¨ã€‚ä¼šåŒæ—¶æ¸…ç†å‘é‡å’Œå›¾è°±ã€‚",
                    "parameters": {
                        "type": "object",
                        "properties": {
                            "content": {"type": "string", "description": "è¦åˆ é™¤çš„å…·ä½“äº‹å®æè¿°"}
                        },
                        "required": ["content"]
                    }
                }
            }
        ]

    # =========================================================================
    # 2. System Prompt
    # =========================================================================
    def _build_system_prompt(self) -> str:
        return """ä½ æ˜¯ä¸€ä¸ªæ‹¥æœ‰åŒå±‚è®°å¿†ç³»ç»Ÿçš„æ™ºèƒ½åŠ©æ‰‹ã€‚

**è®°å¿†æ¶æ„ï¼š**
1. **å±€éƒ¨è®°å¿†**ï¼šå½“å‰å¯¹è¯ä¸Šä¸‹æ–‡ã€‚
2. **å…¨å±€è®°å¿†**ï¼šç”¨æˆ·é•¿æœŸç”»åƒã€‚

**èƒ½åŠ›è¯´æ˜ï¼š**
- ä½ çš„æœç´¢ç»“æœåŒ…å« **æ–‡æœ¬ (Vector)** å’Œ **å›¾è°± (Graph)**ã€‚è¯·ç»“åˆä¸¤è€…å›ç­”ã€‚
- **å›¾è°±ä¼˜å…ˆ**ï¼šå¦‚æœæ–‡æœ¬è®°å½•è¢«åˆ äº†ï¼Œä½†å›¾è°±é‡Œè¿˜æœ‰å…³ç³»ï¼Œè¯´æ˜è®°å¿†å¯èƒ½æœªæ¸…é™¤å¹²å‡€ï¼Œè¯·ä»¥å›¾è°±ä¿¡æ¯ä¸ºè¾…åŠ©å‚è€ƒï¼Œä½†å¦‚æœå›¾è°±æ˜¾ç¤º"Unknown"åˆ™è¡¨ç¤ºç¡®å®ä¸çŸ¥é“ã€‚

**æ“ä½œç­–ç•¥ï¼š**
1. **å­˜å‚¨ (Add)**ï¼šå…¨é‡å­˜å‚¨ã€‚
2. **ä¿®æ­£ (Correction)**ï¼šä¿¡æ¯å˜æ›´æ—¶ï¼Œç›´æ¥ç”¨ `add` è¦†ç›–ã€‚
3. **åˆ é™¤ (Delete)**ï¼šç”¨æˆ·æ˜ç¡®è¦æ±‚åˆ é™¤æ—¶è°ƒç”¨ã€‚
4. **æœç´¢ (Search)**ï¼šå…ˆæœå±€éƒ¨ï¼Œå†æœå…¨å±€ã€‚
"""

    # =========================================================================
    # 3. å·¥å…·æ‰§è¡Œ (å…¨åŸŸåŒæ­¥ä¿®å¤ç‰ˆ)
    # =========================================================================
    def _execute_tool(self, tool_name: str, tool_args: Dict, user_id: int, conversation_id: int, llm_settings: Dict) -> str:
        logger.info(f"ğŸ”§ Agent æ‰§è¡Œå·¥å…·: {tool_name} | å‚æ•°: {tool_args}")
        if not self.memory_manager: return "é”™è¯¯ï¼šè®°å¿†æ¨¡å—æœªåˆå§‹åŒ–ã€‚"

        try:
            def parse_search_result(res):
                vectors = []
                relations = []
                raw_list = []
                if isinstance(res, dict): raw_list = res.get("results", []) or []
                elif isinstance(res, list): raw_list = res
                
                for m in raw_list:
                    if isinstance(m, dict):
                        vectors.append({"id": m.get("id"), "content": m.get("memory") or m.get("text") or str(m)})
                    elif isinstance(m, str):
                        vectors.append({"content": m})

                if isinstance(res, dict) and "relations" in res:
                    for rel in res["relations"]:
                        src = rel.get("source")
                        rel_type = rel.get("relationship")
                        dst = rel.get("destination")
                        if src and rel_type and dst:
                            relations.append(f"{src} --[{rel_type}]--> {dst}")
                return vectors, relations

            # --- åˆ é™¤é€»è¾‘ ---
            if tool_name == "delete_memory":
                query_content = tool_args["content"]
                
                # A. æœç´¢ (åŒ…å«å±€éƒ¨å’Œå…¨å±€ï¼Œä¸”ä¸ä¸¢å¼ƒå›¾è°±)
                candidates = []
                # æœå±€éƒ¨
                local_raw = self.memory_manager.search_memories(query=query_content, user_id=str(user_id), run_id=str(conversation_id), scope='local', limit=10, llm_settings=llm_settings)
                vecs_local, rels_local = parse_search_result(local_raw) # [ä¿®å¤1] ä¹‹å‰æ˜¯ _ï¼Œç°åœ¨æ•è· relations
                for v in vecs_local: 
                    if 'id' in v: candidates.append({"id": v['id'], "content": v['content'], "scope": "å±€éƒ¨"})
                # æŠŠå›¾è°±å…³ç³»ä¹ŸåŠ è¿›å»ï¼Œè®© LLM çŸ¥é“è™½ç„¶å‘é‡æ²¡äº†ä½†å›¾è¿˜åœ¨
                for r in rels_local:
                    candidates.append({"id": "graph_only", "content": f"[å±€éƒ¨å›¾è°±æ®‹ç•™] {r}", "scope": "å±€éƒ¨"})

                # æœå…¨å±€
                global_raw = self.memory_manager.search_memories(query=query_content, user_id=str(user_id), run_id=None, scope='global', limit=10, llm_settings=llm_settings)
                vecs_global, rels_global = parse_search_result(global_raw)
                for v in vecs_global: 
                    if 'id' in v: candidates.append({"id": v['id'], "content": v['content'], "scope": "å…¨å±€"})
                for r in rels_global:
                    candidates.append({"id": "graph_only", "content": f"[å…¨å±€å›¾è°±æ®‹ç•™] {r}", "scope": "å…¨å±€"})

                if not candidates: return f"æœªæ‰¾åˆ°ä¸ '{query_content}' ç›¸å…³çš„è®°å¿†ã€‚"

                # B. å®¡æŸ¥
                reviewer_client = OpenAI(api_key=llm_settings['api_key'], base_url=llm_settings['base_url'])
                review_prompt = f"""
                ç”¨æˆ·æŒ‡ä»¤ï¼šåˆ é™¤ "{query_content}"
                å€™é€‰è®°å¿†ï¼š
                {json.dumps(candidates, ensure_ascii=False, indent=2)}
                
                è¯·åˆ¤æ–­å“ªäº›æ¡ç›®å¿…é¡»åˆ é™¤ï¼Ÿï¼ˆä»…åˆ é™¤äº‹å®åŒ¹é…çš„ï¼‰ã€‚
                è¿”å›IDåˆ—è¡¨ JSONï¼Œå¦‚ ["id1"]ã€‚
                æ³¨æ„ï¼šå¦‚æœæ˜¯ [å›¾è°±æ®‹ç•™] æ¡ç›®ï¼Œä¸éœ€è¦è¿”å›IDï¼ˆå› ä¸ºå®ƒæ²¡æ³•ç›´æ¥åˆ ï¼‰ï¼Œä½†è¿™æ„å‘³ç€æˆ‘ä»¬éœ€è¦æ‰§è¡Œé‡ç½®æ“ä½œã€‚
                """
                try:
                    review_res = reviewer_client.chat.completions.create(
                        model=llm_settings['model_name'], messages=[{"role": "user", "content": review_prompt}], temperature=0
                    )
                    review_content = review_res.choices[0].message.content
                    if "```" in review_content: review_content = re.search(r'```(?:json)?\s*(\[.*?\])\s*```', review_content, re.DOTALL).group(1)
                    ids_to_delete = json.loads(review_content)
                except: ids_to_delete = []

                # C. ç‰©ç†åˆ é™¤ (åˆ é™¤ Vector)
                deleted_contents = []
                for mem_id in ids_to_delete:
                    if mem_id == "graph_only": continue # è·³è¿‡è™šæ‹ŸID
                    target = next((c for c in candidates if c['id'] == mem_id), None)
                    if target:
                        self.memory_manager.delete_memory(mem_id, llm_settings=llm_settings)
                        deleted_contents.append(target['content'])

                # D. å›¾è°±é‡ç½® (å…¨åŸŸåŒæ­¥ä¿®å¤)
                # åªè¦åˆ é™¤äº†ä¸œè¥¿ï¼Œæˆ–è€… LLM å®é™…ä¸Šæ˜¯æƒ³åˆ ä½†åªèƒ½é€šè¿‡é‡ç½®æ¥è§£å†³å›¾è°±æ®‹ç•™
                if deleted_contents or (candidates and not ids_to_delete):
                    neutralize_prompt = f"""
                    ä½ æ˜¯ä¸€ä¸ªçŸ¥è¯†å›¾è°±ä¿®å¤ä¸“å®¶ã€‚ç”¨æˆ·åˆšåˆšåˆ é™¤äº†å…³äº "{query_content}" çš„ä¿¡æ¯ã€‚
                    
                    ä¸ºäº†åˆ‡æ–­å›¾è°±ä¸­çš„æ—§è¿æ¥ï¼Œä½ éœ€è¦ç”Ÿæˆä¸€æ¡â€œé‡ç½®å£°æ˜â€ã€‚
                    
                    ã€ç»å¯¹è§„åˆ™ã€‘
                    1. **ä¸»è¯­å¿…é¡»æ˜¯â€œç”¨æˆ·â€**ï¼šä¸¥ç¦åœ¨å£°æ˜ä¸­å†æ¬¡æåŠè¢«åˆ é™¤çš„å…·ä½“åå­—æˆ–å®ä½“ï¼
                    2. **ä»…é‡ç½®è¢«åˆ å±æ€§**ï¼šåªé‡ç½®è¢«åˆ é™¤çš„é‚£ä¸€é¡¹å±æ€§ã€‚
                    
                    ç¤ºä¾‹ï¼šåˆ é™¤äº†â€œæˆ‘å«å¼ ä¸‰â€ -> è¾“å‡ºï¼šâ€œç”¨æˆ·çš„åå­—æœªçŸ¥â€
                    ç¤ºä¾‹ï¼šåˆ é™¤äº†â€œæˆ‘ä½åœ¨åŒ—äº¬â€ -> è¾“å‡ºï¼šâ€œç”¨æˆ·çš„å±…ä½åœ°æœªçŸ¥â€
                    
                    è¯·ç”Ÿæˆè¿™å¥é‡ç½®å£°æ˜ï¼Œä¸è¦ä»»ä½•å…¶ä»–åºŸè¯ã€‚
                    """
                    try:
                        neutralize_res = reviewer_client.chat.completions.create(
                            model=llm_settings['model_name'], messages=[{"role": "user", "content": neutralize_prompt}], temperature=0
                        )
                        neutral_statement = neutralize_res.choices[0].message.content.strip()
                        
                        # [å…³é”®ä¿®å¤ 2] 1. é‡ç½®å…¨å±€ (Global)
                        self.memory_manager.add_memory(
                            content=neutral_statement,
                            user_id=str(user_id),
                            run_id=None,
                            scope='global',
                            metadata={"type": "graph_reset", "source": "delete_tool"},
                            llm_settings=llm_settings
                        )
                        logger.info(f"ğŸ”„ å›¾è°±é‡ç½®æ‰§è¡Œ (Global): {neutral_statement}")

                        # [å…³é”®ä¿®å¤ 2] 2. é‡ç½®å±€éƒ¨ (Local) - è¿™æ ·å±€éƒ¨å›¾è°±çš„æ—§è¿æ¥ä¹Ÿä¼šè¢« Unknown è¦†ç›–
                        if conversation_id:
                            self.memory_manager.add_memory(
                                content=neutral_statement,
                                user_id=str(user_id),
                                run_id=str(conversation_id),
                                scope='local',
                                metadata={"type": "graph_reset", "source": "delete_tool"},
                                llm_settings=llm_settings
                            )
                            logger.info(f"ğŸ”„ å›¾è°±é‡ç½®æ‰§è¡Œ (Local): {neutral_statement}")

                    except Exception as e:
                        logger.error(f"å›¾è°±é‡ç½®å¤±è´¥: {e}")

                return f"å·²åˆ é™¤ {len(deleted_contents)} æ¡è®°å¿†ï¼Œå¹¶åŒæ­¥æ›´æ–°äº†çŸ¥è¯†å›¾è°±çŠ¶æ€ã€‚"

            # --- å­˜/å–é€»è¾‘ ---
            scope = 'local' if 'local' in tool_name else 'global'
            run_id = str(conversation_id) if scope == 'local' else None
            metadata = {"source_conversation_id": str(conversation_id)} if scope == 'global' else None

            if "add" in tool_name:
                self.memory_manager.add_memory(content=tool_args["content"], user_id=str(user_id), run_id=run_id, scope=scope, metadata=metadata, llm_settings=llm_settings)
                return f"{'å±€éƒ¨' if scope=='local' else 'å…¨å±€'}è®°å¿†å·²æ·»åŠ ã€‚"

            elif "search" in tool_name:
                res = self.memory_manager.search_memories(query=tool_args["query"], user_id=str(user_id), run_id=run_id, scope=scope, limit=5, llm_settings=llm_settings)
                
                logger.info(f"ğŸ” [RAW Search Result] ({scope}): {res}")
                vectors, relations = parse_search_result(res)
                
                output_data = {
                    "relevant_memories": [v['content'] for v in vectors],
                    "knowledge_graph_connections": relations
                }
                
                final_output = f"{'å±€éƒ¨' if scope=='local' else 'å…¨å±€'}æœç´¢ç»“æœ: {json.dumps(output_data, ensure_ascii=False)}"
                logger.info(f"ğŸ“¤ [To LLM]: {final_output}")
                return final_output
            
            return f"æœªçŸ¥å·¥å…·: {tool_name}"

        except Exception as e:
            logger.error(f"å·¥å…·æ‰§è¡Œå¼‚å¸¸: {e}", exc_info=True)
            return f"å·¥å…·æ‰§è¡Œå‡ºé”™: {str(e)}"

    # Agent Loop (ä¿æŒä¸å˜)
    def chat_agent(self, user_id: int, conversation_id: int, user_message: str, history_messages: List[Dict]) -> str:
        client, model_name, llm_settings = self._get_llm_client(user_id)
        if not client: return "è¯·å…ˆé…ç½®æ¨¡å‹ API Keyã€‚"
        messages = [{"role": "system", "content": self._build_system_prompt()}]
        messages.extend(history_messages)
        messages.append({"role": "user", "content": user_message})
        tools = self._get_tools()
        max_turns = 5
        current_turn = 0
        while current_turn < max_turns:
            try:
                response = client.chat.completions.create(model=model_name, messages=messages, tools=tools, tool_choice="auto", temperature=0.7)
                response_message = response.choices[0].message
                if response_message.tool_calls:
                    messages.append(response_message)
                    with concurrent.futures.ThreadPoolExecutor() as executor:
                        futures = []
                        for tool_call in response_message.tool_calls:
                            function_name = tool_call.function.name
                            try: arguments = json.loads(tool_call.function.arguments)
                            except: arguments = {}
                            future = executor.submit(self._execute_tool, function_name, arguments, user_id, conversation_id, llm_settings)
                            futures.append((tool_call, future))
                        for tool_call, future in futures:
                            tool_result = future.result()
                            messages.append({"tool_call_id": tool_call.id, "role": "tool", "name": tool_call.function.name, "content": tool_result})
                    current_turn += 1
                else: return response_message.content
            except Exception as e: return f"å¤„ç†é”™è¯¯: {str(e)}"
        return "æ€è€ƒè¶…æ—¶ã€‚"
        
    # å…¼å®¹æ–¹æ³•
    def delete_conversation_memories(self, *args): pass
    def search_memories(self, *args, **kwargs): return []
    def sync_memory(self, *args, **kwargs): return {}
    def update_memory(self, *args, **kwargs): pass
    def delete_memory(self, *args, **kwargs): pass
    def add_interaction(self, *args, **kwargs): pass
    def _process_message_stream_local(self, *args, **kwargs): pass

agent_service = AgentService()